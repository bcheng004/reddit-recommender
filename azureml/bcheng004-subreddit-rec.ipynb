{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import warnings \n",
        "warnings.simplefilter(action='ignore')\n",
        "!python --version"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Python 3.6.9 :: Anaconda, Inc.\r\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1640221623291
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile conda_dependencies.yml\n",
        "dependencies:\n",
        "    - python=3.6.9\n",
        "    - numpy=1.18.*\n",
        "    - pandas=0.25.*\n",
        "    - pip:\n",
        "        - azureml-defaults==1.34.0\n",
        "        - confuse==1.6.0\n",
        "        - icecream==2.1.1\n",
        "        - plotly==5.3.1\n",
        "        - scikit-surprise==1.1.1\n",
        "        - kaggle==1.5.12"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting conda_dependencies.yml\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "confuse==1.6.0\n",
        "icecream==2.1.1\n",
        "plotly==5.3.1\n",
        "kaleido==0.2.1\n",
        "scikit-surprise==1.1.1\n",
        "kaggle==1.5.12"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting requirements.txt\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Leverage Kaggle API for Subreddit Dataset"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -r requirements.txt"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: confuse==1.6.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.6.0)\nRequirement already satisfied: icecream==2.1.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (2.1.1)\nRequirement already satisfied: plotly==5.3.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (5.3.1)\nRequirement already satisfied: kaleido==0.2.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (0.2.1)\nRequirement already satisfied: scikit-surprise==1.1.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (1.1.1)\nRequirement already satisfied: kaggle==1.5.12 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from -r requirements.txt (line 6)) (1.5.12)\nRequirement already satisfied: pyyaml in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from confuse==1.6.0->-r requirements.txt (line 1)) (6.0)\nRequirement already satisfied: pygments>=2.2.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from icecream==2.1.1->-r requirements.txt (line 2)) (2.10.0)\nRequirement already satisfied: asttokens>=2.0.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from icecream==2.1.1->-r requirements.txt (line 2)) (2.0.5)\nRequirement already satisfied: colorama>=0.3.9 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from icecream==2.1.1->-r requirements.txt (line 2)) (0.4.4)\nRequirement already satisfied: executing>=0.3.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from icecream==2.1.1->-r requirements.txt (line 2)) (0.8.2)\nRequirement already satisfied: six in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from plotly==5.3.1->-r requirements.txt (line 3)) (1.16.0)\nRequirement already satisfied: tenacity>=6.2.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from plotly==5.3.1->-r requirements.txt (line 3)) (8.0.1)\nRequirement already satisfied: joblib>=0.11 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from scikit-surprise==1.1.1->-r requirements.txt (line 5)) (0.14.1)\nRequirement already satisfied: numpy>=1.11.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from scikit-surprise==1.1.1->-r requirements.txt (line 5)) (1.18.5)\nRequirement already satisfied: scipy>=1.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from scikit-surprise==1.1.1->-r requirements.txt (line 5)) (1.5.2)\nRequirement already satisfied: tqdm in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from kaggle==1.5.12->-r requirements.txt (line 6)) (4.62.3)\nRequirement already satisfied: urllib3 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from kaggle==1.5.12->-r requirements.txt (line 6)) (1.25.11)\nRequirement already satisfied: certifi in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from kaggle==1.5.12->-r requirements.txt (line 6)) (2021.10.8)\nRequirement already satisfied: python-slugify in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from kaggle==1.5.12->-r requirements.txt (line 6)) (5.0.2)\nRequirement already satisfied: requests in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from kaggle==1.5.12->-r requirements.txt (line 6)) (2.26.0)\nRequirement already satisfied: python-dateutil in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from kaggle==1.5.12->-r requirements.txt (line 6)) (2.8.2)\nRequirement already satisfied: text-unidecode>=1.3 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from python-slugify->kaggle==1.5.12->-r requirements.txt (line 6)) (1.3)\nRequirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests->kaggle==1.5.12->-r requirements.txt (line 6)) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests->kaggle==1.5.12->-r requirements.txt (line 6)) (2.0.7)\n"
        }
      ],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import confuse\r\n",
        "\r\n",
        "config = confuse.Configuration('redditrec',__name__)\r\n",
        "config.set_file('config-subreddit.yaml')\r\n",
        "data_folder_name = config['folder_paths']['data_folder_name'].get()"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1640221629595
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ```mkdir ~/.kaggle```\r\n",
        "2. upload ```kaggle.json``` and ```mv <source> ~/.kaggle/kaggle.json```\r\n",
        "3. ```chmod 600 ~/.kaggle/kaggle.json```"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "\n",
        "k_api = KaggleApi()\n",
        "k_api.authenticate()"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1639895050854
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from icecream import ic\n",
        "import os\n",
        "\n",
        "if not (os.path.exists(data_folder_name)):\n",
        "    os.mkdir(data_folder_name)\n",
        "os.chdir(data_folder_name)"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1639895052699
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reference = k_api.datasets_list(search='Subreddit Recommender')[0]['ref']\n",
        "reference = 'timschaum/subreddit-recommender'\n",
        "ic(reference);\n",
        "k_api.dataset_download_files(reference)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "ic| reference: 'timschaum/subreddit-recommender'\n"
        }
      ],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1639890905640
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "zf_name = reference.split('/')[1] + '.zip'\n",
        "zf = ZipFile(zf_name)\n",
        "zf.extractall()\n",
        "zf.close()"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1639890924692
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_dir = os.path.dirname(os.getcwd())\n",
        "os.chdir(original_dir)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1639895058717
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\r\n",
        "from IPython.display import display\r\n",
        "from icecream import ic\r\n",
        "\r\n",
        "subreddit_df = pd.read_csv(os.path.join(data_folder_name,'reddit_user_data_count.csv'))\r\n",
        "# limiting user number for compute limitations\r\n",
        "# display(list(subreddit_df['user'].unique()[:100]))\r\n",
        "subreddit_df = subreddit_df[subreddit_df['user'].isin(list(subreddit_df['user'].unique()[:100]))]\r\n",
        "display(subreddit_df.head())\r\n",
        "ic(subreddit_df.shape);"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "                   user        subreddit  count\n0  ------Username------        AskReddit     20\n1  ------Username------            Barca      9\n2  ------Username------             FIFA      4\n3  ------Username------              MMA      5\n4  ------Username------  RioGrandeValley      3",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user</th>\n      <th>subreddit</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>------Username------</td>\n      <td>AskReddit</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>------Username------</td>\n      <td>Barca</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>------Username------</td>\n      <td>FIFA</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>------Username------</td>\n      <td>MMA</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>------Username------</td>\n      <td>RioGrandeValley</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "ic| subreddit_df.shape: (4201, 3)\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1640221643270
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\r\n",
        "\r\n",
        "plot_output_folder = config['folder_paths']['plot_output_folder_name'].get()\r\n",
        "if not os.path.exists(plot_output_folder):\r\n",
        "    os.mkdir(plot_output_folder)\r\n",
        "group_col, count_col, top_n = 'subreddit', 'count', 5\r\n",
        "hist_df = subreddit_df.groupby(by=[group_col])[count_col].sum()\\\r\n",
        ".sort_values(ascending=False)[0:top_n]\r\n",
        "hist_trace = go.Bar(\r\n",
        "    x = hist_df.index,\r\n",
        "    text = [f'{val:.1f}' for val in hist_df.values],\r\n",
        "    textposition = 'auto',\r\n",
        "    textfont = dict(color='blue'),\r\n",
        "    y = hist_df.values\r\n",
        ")\r\n",
        "hist_layout = dict(\r\n",
        "    title = f'Distribution of Top {top_n} {group_col}',\r\n",
        "    xaxis = dict(title=f'{group_col}'),\r\n",
        "    yaxis = dict(title='count')\r\n",
        ")\r\n",
        "fig = go.Figure(data=[hist_trace],layout=hist_layout)\r\n",
        "fig.write_image(os.path.join(plot_output_folder,f\"top_{top_n}_eda.png\"))"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639974922075
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Azureml Workspace and Experiment"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import azureml.core\r\n",
        "import numpy as np\r\n",
        "import logging, json\r\n",
        "from azureml.core import Workspace, Dataset, Experiment, Environment, Run\r\n",
        "\r\n",
        "ws = Workspace.from_config()\r\n",
        "experiment_name = config['azureml']['experiment_name'].get()\r\n",
        "experiment = Experiment(ws,name=experiment_name)\r\n",
        "env_name = config['azureml']['environment_name'].get()\r\n",
        "subreddit_env = Environment.from_conda_specification(\r\n",
        "    name=env_name,\r\n",
        "    file_path='./conda_dependencies.yml'\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1640223142191
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Register dataset"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datastore = ws.get_default_datastore()\r\n",
        "dataset_name = config['azureml']['dataset_name'].get()\r\n",
        "dataset = Dataset.Tabular.register_pandas_dataframe(\r\n",
        "    subreddit_df,\r\n",
        "    datastore,\r\n",
        "    dataset_name,\r\n",
        "    show_progress=True\r\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Validating arguments.\nArguments validated.\nSuccessfully obtained datastore reference and path.\nUploading file to managed-dataset/d76ea2f3-1e88-4769-9788-cd485240bb43/\nSuccessfully uploaded file to datastore.\nCreating and registering a new dataset.\nSuccessfully created and registered a new dataset.\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1640039417665
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test registered dataset"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subreddit_df = Dataset.get_by_name(ws,name=dataset_name).to_pandas_dataframe()\r\n",
        "display(subreddit_df.head())"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "                   user        subreddit  count\n0  ------Username------        AskReddit     20\n1  ------Username------            Barca      9\n2  ------Username------             FIFA      4\n3  ------Username------              MMA      5\n4  ------Username------  RioGrandeValley      3",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user</th>\n      <th>subreddit</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>------Username------</td>\n      <td>AskReddit</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>------Username------</td>\n      <td>Barca</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>------Username------</td>\n      <td>FIFA</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>------Username------</td>\n      <td>MMA</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>------Username------</td>\n      <td>RioGrandeValley</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1640039422108
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fit models leveraging **scikit-surprise**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import surprise\r\n",
        "from surprise import (\r\n",
        "    # collaborative filtering models\r\n",
        "    NormalPredictor, BaselineOnly, KNNBasic, KNNWithMeans,\r\n",
        "    KNNWithZScore, KNNBaseline, SVD, SlopeOne, CoClustering\r\n",
        ")\r\n",
        "from surprise.accuracy import (\r\n",
        "    rmse, mae, mse, fcp\r\n",
        ")\r\n",
        "from surprise.model_selection import train_test_split\r\n",
        "\r\n",
        "random_state = config['surprise']['random_state'].get()\r\n",
        "verbose_bool = config['surprise']['verbose_bool'].get()\r\n",
        "test_size = config['surprise']['test_size'].get()\r\n",
        "max_count, min_count = max(subreddit_df['count']), min(subreddit_df['count'])\r\n",
        "reader = surprise.Reader(rating_scale=(min_count,max_count))\r\n",
        "full_data = surprise.Dataset.load_from_df(subreddit_df,reader)\r\n",
        "train_data, test_data = train_test_split(full_data,test_size=test_size,random_state=random_state)\r\n",
        "model_list = [\r\n",
        "    NormalPredictor(),\r\n",
        "    KNNBasic(k=40, min_k=1, verbose=verbose_bool),\r\n",
        "    KNNWithMeans(k=40, min_k=1, verbose=verbose_bool),\r\n",
        "    KNNWithZScore(k=40, min_k=1, verbose=verbose_bool),\r\n",
        "    KNNBaseline(k=40, min_k=1, verbose=verbose_bool),\r\n",
        "    SVD(n_factors=100, n_epochs=20, biased=True, init_mean=0,\r\n",
        "    init_std_dev=.1, lr_all=.005,reg_all=.02, lr_bu=None, lr_bi=None, \r\n",
        "    lr_pu=None, lr_qi=None,reg_bu=None, reg_bi=None, reg_pu=None, reg_qi=None,\r\n",
        "    random_state=random_state, verbose=verbose_bool),\r\n",
        "    SlopeOne(),\r\n",
        "    CoClustering(n_cltr_u=3, n_cltr_i=3, n_epochs=20, random_state=random_state,\r\n",
        "    verbose=verbose_bool)\r\n",
        "]\r\n",
        "benchmark = []\r\n",
        "for model in model_list:\r\n",
        "    model.fit(train_data)\r\n",
        "    y_pred = model.test(test_data)\r\n",
        "    model_dict = {\r\n",
        "        'model': str(model).split(' ')[0].split('.')[-1],\r\n",
        "        'rmse': rmse(y_pred,verbose=verbose_bool),\r\n",
        "        'mse': mse(y_pred,verbose=verbose_bool),\r\n",
        "        'mae': mae(y_pred,verbose=verbose_bool),\r\n",
        "        'fcp': fcp(y_pred,verbose=verbose_bool)\r\n",
        "    }\r\n",
        "    benchmark.append(model_dict)\r\n",
        "benchmark_df = pd.DataFrame(benchmark)\r\n",
        "display(benchmark_df)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "             model        rmse            mse         mae       fcp\n0  NormalPredictor   63.453432    4026.338093   31.997717  0.444297\n1         KNNBasic   63.050422    3975.355760   17.601135  0.441253\n2     KNNWithMeans   60.240092    3628.868744   18.342302  0.398839\n3    KNNWithZScore   56.452901    3186.930065   17.430292  0.417661\n4      KNNBaseline   61.730841    3810.696700   17.328740  0.375936\n5              SVD  950.393987  903248.731087  945.669678  0.011393\n6         SlopeOne   61.845622    3824.880995   17.950825  0.402733\n7     CoClustering   68.400337    4678.606141   19.032222  0.370516",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>rmse</th>\n      <th>mse</th>\n      <th>mae</th>\n      <th>fcp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NormalPredictor</td>\n      <td>63.453432</td>\n      <td>4026.338093</td>\n      <td>31.997717</td>\n      <td>0.444297</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>KNNBasic</td>\n      <td>63.050422</td>\n      <td>3975.355760</td>\n      <td>17.601135</td>\n      <td>0.441253</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>KNNWithMeans</td>\n      <td>60.240092</td>\n      <td>3628.868744</td>\n      <td>18.342302</td>\n      <td>0.398839</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>KNNWithZScore</td>\n      <td>56.452901</td>\n      <td>3186.930065</td>\n      <td>17.430292</td>\n      <td>0.417661</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>KNNBaseline</td>\n      <td>61.730841</td>\n      <td>3810.696700</td>\n      <td>17.328740</td>\n      <td>0.375936</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>SVD</td>\n      <td>950.393987</td>\n      <td>903248.731087</td>\n      <td>945.669678</td>\n      <td>0.011393</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>SlopeOne</td>\n      <td>61.845622</td>\n      <td>3824.880995</td>\n      <td>17.950825</td>\n      <td>0.402733</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>CoClustering</td>\n      <td>68.400337</td>\n      <td>4678.606141</td>\n      <td>19.032222</td>\n      <td>0.370516</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1640221660969
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KNNWithZScore Model performs the best on the test data based off:\r\n",
        "\r\n",
        "* rmse\r\n",
        "* mse"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import AmlCompute, ComputeTarget\r\n",
        "\r\n",
        "amlcompute_cluster_name = config['azureml']['compute_cluster']['name'].get()\r\n",
        "provisioning_config = AmlCompute.provisioning_configuration(\r\n",
        "    vm_size=config['azureml']['compute_cluster']['vm_size'].get(),\r\n",
        "    max_nodes=config['azureml']['compute_cluster']['max_nodes'].get()\r\n",
        ")\r\n",
        "compute_target = ComputeTarget.create(ws,amlcompute_cluster_name,provisioning_config)\r\n",
        "compute_target.wait_for_completion(\r\n",
        "    show_output=True,\r\n",
        "    min_node_count=None,\r\n",
        "    timeout_in_minutes=20\r\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "SucceededProvisioning operation finished, operation \"Succeeded\"\nSucceeded\nAmlCompute wait for completion finished\n\nMinimum number of nodes requested have been provisioned\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1640221675024
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import ScriptRunConfig\r\n",
        "\r\n",
        "src = ScriptRunConfig(\r\n",
        "    source_directory='.',\r\n",
        "    script='./train_exp/train_rec.py',\r\n",
        "    arguments=[\r\n",
        "        '--k',40,\r\n",
        "        '--min_k',1,\r\n",
        "        '--verbose',verbose_bool\r\n",
        "    ],\r\n",
        "    compute_target=compute_target,\r\n",
        "    environment=subreddit_env\r\n",
        ")\r\n",
        "exp_run = experiment.submit(src)\r\n",
        "exp_run.wait_for_completion(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "RunId: redditrecsys_1640040863_7001b78d\nWeb View: https://ml.azure.com/runs/redditrecsys_1640040863_7001b78d?wsid=/subscriptions/65881521-f775-4359-a9d9-a122ba465711/resourcegroups/redditrecrg/workspaces/redditrecws&tid=f26668e5-2944-4a48-abc0-4060b08570e2\n\nExecution Summary\n=================\nRunId: redditrecsys_1640040863_7001b78d\nWeb View: https://ml.azure.com/runs/redditrecsys_1640040863_7001b78d?wsid=/subscriptions/65881521-f775-4359-a9d9-a122ba465711/resourcegroups/redditrecrg/workspaces/redditrecws&tid=f26668e5-2944-4a48-abc0-4060b08570e2\n\nWarnings:\nThis run might be using a new job runtime with improved performance and error reporting. The logs from your script are in user_logs/std_log.txt. Please let us know if you run into any issues, and if you would like to opt-out, please add the environment variable AZUREML_COMPUTE_USE_COMMON_RUNTIME to the environment variables section of the job and set its value to the string \"false\"\n\n\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {
            "text/plain": "{'runId': 'redditrecsys_1640040863_7001b78d',\n 'target': 'cpu-cluster',\n 'status': 'Completed',\n 'startTimeUtc': '2021-12-20T22:54:41.316764Z',\n 'endTimeUtc': '2021-12-20T22:54:58.509775Z',\n 'services': {},\n 'warnings': [{'message': 'This run might be using a new job runtime with improved performance and error reporting. The logs from your script are in user_logs/std_log.txt. Please let us know if you run into any issues, and if you would like to opt-out, please add the environment variable AZUREML_COMPUTE_USE_COMMON_RUNTIME to the environment variables section of the job and set its value to the string \"false\"'}],\n 'properties': {'_azureml.ComputeTargetType': 'amlcompute',\n  'ContentSnapshotId': '78ac5550-c3ef-4890-bc12-c25d2377d9ab',\n  'azureml.git.repository_uri': 'git@github.com:bcheng004/reddit-recommender.git',\n  'mlflow.source.git.repoURL': 'git@github.com:bcheng004/reddit-recommender.git',\n  'azureml.git.branch': 'main',\n  'mlflow.source.git.branch': 'main',\n  'azureml.git.commit': 'da6ea7403a08f0b64ae0522fcf2246da7c2e0755',\n  'mlflow.source.git.commit': 'da6ea7403a08f0b64ae0522fcf2246da7c2e0755',\n  'azureml.git.dirty': 'True',\n  'ProcessInfoFile': 'azureml-logs/process_info.json',\n  'ProcessStatusFile': 'azureml-logs/process_status.json'},\n 'inputDatasets': [{'dataset': {'id': '0d50a630-e809-4c92-83be-65d6cac08cca'}, 'consumptionDetails': {'type': 'Reference'}}],\n 'outputDatasets': [],\n 'runDefinition': {'script': 'train_exp/train_rec.py',\n  'command': '',\n  'useAbsolutePath': False,\n  'arguments': ['--k', '40', '--min_k', '1', '--verbose', 'False'],\n  'sourceDirectoryDataStore': None,\n  'framework': 'Python',\n  'communicator': 'None',\n  'target': 'cpu-cluster',\n  'dataReferences': {},\n  'data': {},\n  'outputData': {},\n  'datacaches': [],\n  'jobName': None,\n  'maxRunDurationSeconds': 2592000,\n  'nodeCount': 1,\n  'instanceTypes': [],\n  'priority': None,\n  'credentialPassthrough': False,\n  'identity': None,\n  'environment': {'name': 'reddit-env',\n   'version': 'Autosave_2021-12-20T04:39:38Z_38353f3e',\n   'python': {'interpreterPath': 'python',\n    'userManagedDependencies': False,\n    'condaDependencies': {'dependencies': ['python=3.6.9',\n      'numpy=1.18.*',\n      'pandas=0.25.*',\n      {'pip': ['azureml-defaults==1.34.0',\n        'confuse==1.6.0',\n        'icecream==2.1.1',\n        'plotly==5.3.1',\n        'scikit-surprise==1.1.1',\n        'kaggle==1.5.12']}],\n     'name': 'azureml_4d6fb67438b6f4b0907f09244d116756'},\n    'baseCondaEnvironment': None},\n   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n   'docker': {'baseImage': 'mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20211029.v1',\n    'platform': {'os': 'Linux', 'architecture': 'amd64'},\n    'baseDockerfile': None,\n    'baseImageRegistry': {'address': None, 'username': None, 'password': None},\n    'enabled': False,\n    'arguments': []},\n   'spark': {'repositories': [], 'packages': [], 'precachePackages': True},\n   'inferencingStackVersion': None},\n  'history': {'outputCollection': True,\n   'directoriesToWatch': ['logs'],\n   'enableMLflowTracking': True,\n   'snapshotProject': True},\n  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n    'spark.yarn.maxAppAttempts': '1'}},\n  'parallelTask': {'maxRetriesPerWorker': 0,\n   'workerCountPerNode': 1,\n   'terminalExitCodes': None,\n   'configuration': {}},\n  'amlCompute': {'name': None,\n   'vmSize': None,\n   'retainCluster': False,\n   'clusterMaxNodeCount': None},\n  'aiSuperComputer': {'instanceType': 'D2',\n   'imageVersion': 'pytorch-1.7.0',\n   'location': None,\n   'aiSuperComputerStorageData': None,\n   'interactive': False,\n   'scalePolicy': None,\n   'virtualClusterArmId': None,\n   'tensorboardLogDirectory': None,\n   'sshPublicKey': None,\n   'sshPublicKeys': None,\n   'enableAzmlInt': True,\n   'priority': 'Medium',\n   'slaTier': 'Standard',\n   'userAlias': None},\n  'kubernetesCompute': {'instanceType': None},\n  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n  'mpi': {'processCountPerNode': 1},\n  'pyTorch': {'communicationBackend': 'nccl', 'processCount': None},\n  'hdi': {'yarnDeployMode': 'Cluster'},\n  'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5},\n  'exposedPorts': None,\n  'docker': {'useDocker': False,\n   'sharedVolumes': True,\n   'shmSize': '2g',\n   'arguments': []},\n  'cmk8sCompute': {'configuration': {}},\n  'commandReturnCodeConfig': {'returnCode': 'Zero',\n   'successfulReturnCodes': []},\n  'environmentVariables': {},\n  'applicationEndpoints': {},\n  'parameters': []},\n 'logFiles': {'logs/azureml/18_azureml.log': 'https://redditrecws5030366321.blob.core.windows.net/azureml/ExperimentRun/dcid.redditrecsys_1640040863_7001b78d/logs/azureml/18_azureml.log?sv=2019-07-07&sr=b&sig=R5OeLU4YzY15Kf3mETQm3lo6%2BaOUSSYb63j%2B4DoV91w%3D&skoid=9e0980dd-f2f0-4a0a-bf80-201add4e42d3&sktid=f26668e5-2944-4a48-abc0-4060b08570e2&skt=2021-12-20T22%3A44%3A58Z&ske=2021-12-22T06%3A54%3A58Z&sks=b&skv=2019-07-07&st=2021-12-20T22%3A44%3A58Z&se=2021-12-21T06%3A54%3A58Z&sp=r',\n  'logs/azureml/dataprep/backgroundProcess.log': 'https://redditrecws5030366321.blob.core.windows.net/azureml/ExperimentRun/dcid.redditrecsys_1640040863_7001b78d/logs/azureml/dataprep/backgroundProcess.log?sv=2019-07-07&sr=b&sig=C5gD2UcAvwaXxyJO1Du8mbH0BO%2Bjfv97Pa1OPvhG6yA%3D&skoid=9e0980dd-f2f0-4a0a-bf80-201add4e42d3&sktid=f26668e5-2944-4a48-abc0-4060b08570e2&skt=2021-12-20T22%3A44%3A58Z&ske=2021-12-22T06%3A54%3A58Z&sks=b&skv=2019-07-07&st=2021-12-20T22%3A44%3A58Z&se=2021-12-21T06%3A54%3A58Z&sp=r',\n  'logs/azureml/dataprep/backgroundProcess_Telemetry.log': 'https://redditrecws5030366321.blob.core.windows.net/azureml/ExperimentRun/dcid.redditrecsys_1640040863_7001b78d/logs/azureml/dataprep/backgroundProcess_Telemetry.log?sv=2019-07-07&sr=b&sig=IBa%2BBoO3H9Kf5E6Bpu%2FZUoZud8%2BMCgHUJkkvQ9BjGO0%3D&skoid=9e0980dd-f2f0-4a0a-bf80-201add4e42d3&sktid=f26668e5-2944-4a48-abc0-4060b08570e2&skt=2021-12-20T22%3A44%3A58Z&ske=2021-12-22T06%3A54%3A58Z&sks=b&skv=2019-07-07&st=2021-12-20T22%3A44%3A58Z&se=2021-12-21T06%3A54%3A58Z&sp=r',\n  'logs/azureml/dataprep/rslex.log': 'https://redditrecws5030366321.blob.core.windows.net/azureml/ExperimentRun/dcid.redditrecsys_1640040863_7001b78d/logs/azureml/dataprep/rslex.log?sv=2019-07-07&sr=b&sig=AtZvymEVIY0iJt7olEcyEQjGGuYv8Brc8PXkHxiDDO8%3D&skoid=9e0980dd-f2f0-4a0a-bf80-201add4e42d3&sktid=f26668e5-2944-4a48-abc0-4060b08570e2&skt=2021-12-20T22%3A44%3A58Z&ske=2021-12-22T06%3A54%3A58Z&sks=b&skv=2019-07-07&st=2021-12-20T22%3A44%3A58Z&se=2021-12-21T06%3A54%3A58Z&sp=r'},\n 'submittedBy': 'Bohao Cheng'}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1640040899042
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exp_run = Run(experiment,run_id='redditrecsys_1640040863_7001b78d')\r\n",
        "model_output_folder = config['azureml']['model_output_folder'].get()\r\n",
        "model_path = [item for item in exp_run.get_file_names() if model_output_folder in item][0]\r\n",
        "model_name = config['azureml']['model_name'].get()\r\n",
        "model = exp_run.register_model(model_name=model_name,model_path=model_path)"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1640221745366
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.model import InferenceConfig, Model\r\n",
        "\r\n",
        "model_check = Model(ws,model_name)\r\n",
        "model_check.download(target_dir=os.getcwd(),exist_ok=True)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "'/mnt/batch/tasks/shared/LS_root/mounts/clusters/redditreccompute/code/Users/bcheng004/reddit-recommender/azureml/model.joblib'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1640221756028
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.webservice import AciWebservice, Webservice\r\n",
        "\r\n",
        "script_file_name = config['azureml']['inference_script_path'].get()\r\n",
        "inference_config = InferenceConfig(\r\n",
        "    entry_script = script_file_name,\r\n",
        "    environment=exp_run.get_environment()\r\n",
        ")\r\n",
        "aciconfig = AciWebservice.deploy_configuration(\r\n",
        "    cpu_cores=config['azureml']['aciservice']['cpu_cores'].get(),\r\n",
        "    memory_gb=config['azureml']['aciservice']['memory_gb'].get()\r\n",
        ")\r\n",
        "aci_service_name = config['azureml']['aciservice']['endpoint_name'].get()\r\n",
        "aci_service = Model.deploy(\r\n",
        "    ws,\r\n",
        "    aci_service_name,\r\n",
        "    [model],\r\n",
        "    inference_config,\r\n",
        "    aciconfig\r\n",
        ")\r\n",
        "aci_service.wait_for_deployment(True)\r\n",
        "print(aci_service.state)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\nRunning\n2021-12-23 01:09:24+00:00 Creating Container Registry if not exists.\n2021-12-23 01:09:24+00:00 Registering the environment.\n2021-12-23 01:09:27+00:00 Use the existing image.\n2021-12-23 01:09:27+00:00 Generating deployment configuration.\n2021-12-23 01:09:28+00:00 Submitting deployment to compute.\n2021-12-23 01:09:32+00:00 Checking the status of deployment subreddit-api..\n2021-12-23 01:12:41+00:00 Checking the status of inference endpoint subreddit-api.\nSucceeded\nACI service creation operation finished, operation \"Succeeded\"\nHealthy\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1640222465981
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_user_count = subreddit_df['user'].value_counts().sort_values(ascending=False)[:1]\r\n",
        "top_user = list(top_user_count.index)[0]\r\n",
        "unique_items = subreddit_df['subreddit'].unique()\r\n",
        "item_user_x = subreddit_df.loc[(subreddit_df['user']==top_user),'subreddit']\r\n",
        "items_to_pred = np.setdiff1d(unique_items,item_user_x)"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1640222977790
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint_service = AciWebservice(ws,name=aci_service_name)\r\n",
        "recom_list = []\r\n",
        "for iid in items_to_pred:\r\n",
        "    pred_df = pd.DataFrame(columns=['user','subreddit'])\r\n",
        "    pred_df.loc[0] = [top_user,iid]\r\n",
        "    x_pred = json.dumps({'data': pred_df.to_dict(orient='records')})\r\n",
        "    pred_response = endpoint_service.run(input_data=x_pred)\r\n",
        "    recom_list.append(json.loads(pred_response))"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1640223278343
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_recom_list = [pred['prediction'] for pred in recom_list]\r\n",
        "pred_recom_df = pd.DataFrame(cleaned_recom_list)\r\n",
        "top_10_recom = pred_recom_df.sort_values('est',ascending=False)[:10]\r\n",
        "display(top_10_recom)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "          uid                 iid        est  \\\n1249  -j4ckK-            brighton  40.720899   \n226   -j4ckK-    CallOfDutyMobile  39.978305   \n1430  -j4ckK-          fordranger  39.752490   \n1231  -j4ckK-          boardgames  38.468870   \n889   -j4ckK-           Rochester  32.935398   \n67    -j4ckK-       AnarchoGaming  32.406958   \n345   -j4ckK-       DenverBroncos  30.704654   \n1880  -j4ckK-              soccer  27.316266   \n734   -j4ckK-               NBA2k  26.645153   \n914   -j4ckK-  SchoolIdolFestival  26.527418   \n\n                                       details  \n1249  {'actual_k': 2, 'was_impossible': False}  \n226   {'actual_k': 1, 'was_impossible': False}  \n1430  {'actual_k': 1, 'was_impossible': False}  \n1231  {'actual_k': 3, 'was_impossible': False}  \n889   {'actual_k': 1, 'was_impossible': False}  \n67    {'actual_k': 1, 'was_impossible': False}  \n345   {'actual_k': 1, 'was_impossible': False}  \n1880  {'actual_k': 4, 'was_impossible': False}  \n734   {'actual_k': 1, 'was_impossible': False}  \n914   {'actual_k': 1, 'was_impossible': False}  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>uid</th>\n      <th>iid</th>\n      <th>est</th>\n      <th>details</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1249</th>\n      <td>-j4ckK-</td>\n      <td>brighton</td>\n      <td>40.720899</td>\n      <td>{'actual_k': 2, 'was_impossible': False}</td>\n    </tr>\n    <tr>\n      <th>226</th>\n      <td>-j4ckK-</td>\n      <td>CallOfDutyMobile</td>\n      <td>39.978305</td>\n      <td>{'actual_k': 1, 'was_impossible': False}</td>\n    </tr>\n    <tr>\n      <th>1430</th>\n      <td>-j4ckK-</td>\n      <td>fordranger</td>\n      <td>39.752490</td>\n      <td>{'actual_k': 1, 'was_impossible': False}</td>\n    </tr>\n    <tr>\n      <th>1231</th>\n      <td>-j4ckK-</td>\n      <td>boardgames</td>\n      <td>38.468870</td>\n      <td>{'actual_k': 3, 'was_impossible': False}</td>\n    </tr>\n    <tr>\n      <th>889</th>\n      <td>-j4ckK-</td>\n      <td>Rochester</td>\n      <td>32.935398</td>\n      <td>{'actual_k': 1, 'was_impossible': False}</td>\n    </tr>\n    <tr>\n      <th>67</th>\n      <td>-j4ckK-</td>\n      <td>AnarchoGaming</td>\n      <td>32.406958</td>\n      <td>{'actual_k': 1, 'was_impossible': False}</td>\n    </tr>\n    <tr>\n      <th>345</th>\n      <td>-j4ckK-</td>\n      <td>DenverBroncos</td>\n      <td>30.704654</td>\n      <td>{'actual_k': 1, 'was_impossible': False}</td>\n    </tr>\n    <tr>\n      <th>1880</th>\n      <td>-j4ckK-</td>\n      <td>soccer</td>\n      <td>27.316266</td>\n      <td>{'actual_k': 4, 'was_impossible': False}</td>\n    </tr>\n    <tr>\n      <th>734</th>\n      <td>-j4ckK-</td>\n      <td>NBA2k</td>\n      <td>26.645153</td>\n      <td>{'actual_k': 1, 'was_impossible': False}</td>\n    </tr>\n    <tr>\n      <th>914</th>\n      <td>-j4ckK-</td>\n      <td>SchoolIdolFestival</td>\n      <td>26.527418</td>\n      <td>{'actual_k': 1, 'was_impossible': False}</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1640223405822
        }
      }
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "961fee12dacd1373a18d1b8aa10b592e6d87f8713f5cf1b9a971545ca71e0b61"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}